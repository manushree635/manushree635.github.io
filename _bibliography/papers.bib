---
---
@article{smoothdiff,
  abbr= {Smooth Diffusion},
  author={{Guo*}, Jiayi and {Xu*}, Xingqian and Pu, Yifan and Wang, Chaofei and Vasu, Manushree and Song, Shiji and Shi, Humphrey},
  title={{Smooth Diffusion: Crafting Smooth Latent Spaces in Diffusion Models}},
  journal={CVPR},
  year={2024},
  selected={true},
  arxiv={2312.04410},
  code={https://github.com/SHI-Labs/Smooth-Diffusion},
  html={https://shi-labs.github.io/Smooth-Diffusion/},
  img={https://shi-labs.github.io/Smooth-Diffusion/static/videos/Smooth%20diff-nonloop-0319.mp4},
}

@article{coprompter,
  abbr= {CoPrompter},
  author={{Vasu* et al.}, Manushree},
  title={{CoPrompter: User-Centric Evaluation of LLM Instruction Alignment for Improved Prompt Engineering}},
  journal={Under Review},
  year={2024},
  selected={true},
  img={assets/img/publications/hssd.gif}
}

@article{cvqa,
  abbr= {Continual VQA},
  author={{Vasu}, Manushree and {Kane}, Aditya and {Khose}, Sahil},
  title={{Continual VQA for Disaster Response Systems}},
  journal={NeurIPS Workshop (Tackling Climate Change with Machine Learning)},
  year={2023},
  selected={true},
  arxiv={2209.10320},
  img={assets/img/publications/goat.gif},
}


@article{homerobotovmm,
  abbr= {OVMM},
  title         =     {HomeRobot: Open Vocab Mobile Manipulation ü§ñ},
  author        =     {Sriram Yenamandra and Arun Ramachandran and Karmesh Yadav and Austin Wang and Mukul Khanna and Theophile Gervet and Tsung-Yen Yang and Vidhi Jain and Alex William Clegg and John Turner and Zsolt Kira and Manolis Savva and Angel Chang and Devendra Singh Chaplot and Dhruv Batra and Roozbeh Mottaghi and Yonatan Bisk and Chris Paxton},
  url           =     {https://aihabitat.org/static/challenge/home_robot_ovmm_2023/OVMM.pdf},
  year          =     {2023},
  selected={true},
  journal={Conference on Robot Learning (CoRL), NeurIPS Competition Track},
  img={assets/img/publications/ovmm.gif},
  arxiv={2306.11565},
  code={https://github.com/facebookresearch/home-robot},
  html={https://ovmm.github.io}
}


@INPROCEEDINGS{emqa,
  abbr= {EMQA},
  author={Datta, Samyak and Dharur, Sameer and Cartillier, Vincent and Desai, Ruta and Khanna, Mukul and Batra, Dhruv and Parikh, Devi},
  year={2022},
  booktitle={CVPR},
  title={Episodic Memory Question Answering ü§ñ üéûÔ∏è},
  volume={},
  number={},
  selected={true},
  html={https://samyak-268.github.io/emqa},
  pdf={https://openaccess.thecvf.com/content/CVPR2022/papers/Datta_Episodic_Memory_Question_Answering_CVPR_2022_paper.pdf},
  img={assets/img/publications/emqa2.png},
  doi={}}

@INPROCEEDINGS{emqa,
  abbr= {DeepHS-HDRV},
  author={Khan, Zeeshan and Shettiwar, Parth and Khanna, Mukul and Raman, Shanmuganathan},
  year={2022},
  booktitle={International Conference on Pattern Recognition (ICPR)},
  title={DeepHS-HDRVideo: Deep High Speed High Dynamic Range Video Reconstruction üì∏},
  volume={},
  selected={true},
  number={},
  arxiv={https://arxiv.org/pdf/2210.04429},
  doi={},
  img={assets/img/publications/deephdrv.gif}
  }

@INPROCEEDINGS{9469450,
  abbr= {BF2NormalNet},
  author={Khanna, Mukul and Sharma, Tanu and Thatavarthy, Ayyappa Swamy and Krishna, K. Madhava},
  year={2021},
  abstract={Surface normal estimation is an essential component of several computer and robot vision pipelines. While this problem has been extensively studied, most approaches are geared towards indoor scenes and often rely on multiple modalities (depth, multiple views) for accurate estimation of normal maps. Outdoor scenes pose a greater challenge as they exhibit significant lighting variation, often contain occluders, and structures like building facades are often ridden with numerous windows and protrusions. Conventional supervised learning schemes excel in indoor scenes, but do not exhibit competitive performance when trained and deployed in outdoor environments. Furthermore, they involve complex network architectures and require many more trainable parameters. To tackle these challenges, we present an adversarial learning scheme that regularizes the output normal maps from a neural network to appear more realistic, by using a small number of precisely annotated examples. Our method presents a lightweight and simpler architecture, while improving performance by at least 1.5x across most metrics. We evaluate our approaches against the state-of-the-art on normal map estimation, on a synthetic and a real outdoor dataset, and observe significant performance enhancements.},
  booktitle={Conference on Robots and Vision (CRV)},
  title={Building Facades to Normal Maps: Adversarial Learning from Single View Images üè¢},
  volume={},
  number={},
  selected={true},
  pages={33-40},
  code={https://github.com/mukulkhanna/bf2normalnet},
  html={/building-facade-normal-estimation-crv},
  img={assets/img/publications/teaser.png},
  doi={10.1109/CRV52889.2021.00009}},
  }

@INPROCEEDINGS{8969167,
  abbr= {FHDR},
  author={Khan, Zeeshan and Khanna, Mukul and Raman, Shanmuganathan},
  booktitle={GlobalSIP}, 
  title={FHDR: HDR Image Reconstruction from a Single LDR Image using Feedback Network üì∏}, 
  year={2019},
  volume={},
  number={},
  selected={true},
  pages={1-5},
  abstract={High dynamic range (HDR) image generation from a single exposure low dynamic range (LDR) image has been made possible due to the recent advances in Deep Learning. Various feed-forward Convolutional Neural Networks (CNNs) have been proposed for learning LDR to HDR representations. To better utilize the power of CNNs, we exploit the idea of feedback, where the initial low level features are guided by the high level features using a hidden state of a Recurrent Neural Network. Unlike a single forward pass in a conventional feed-forward network, the reconstruction from LDR to HDR in a feedback network is learned over multiple iterations. This enables us to create a coarse-to-fine representation, leading to an improved reconstruction at every iteration. Various advantages over standard feed-forward networks include early reconstruction ability and better reconstruction quality with fewer network parameters. We design a dense feedback block and propose an end-to-end feedback network-FHDR for HDR image generation from a single exposure LDR image. Qualitative and quantitative evaluations show the superiority of our approach over the state-of-the-art methods.},
  keywords={HDR imaging;Feedback Networks;RNN;Deep Learning},
  doi={10.1109/GlobalSIP45357.2019.8969167},
  ISSN={},
  code={http://github.com/mukulkhanna/fhdr},
  arxiv={1912.11463},
  img={assets/img/publications/fhdr.png},
}

@INPROCEEDINGS{8734309,
  abbr= {URSIM},
  author={Katara, Pushkal and Khanna, Mukul and Nagar, Harshit and Panaiyappan, A.},
  booktitle={Underwater Technology (UT)}, 
  title={Open Source Simulator for Unmanned Underwater Vehicles using ROS and Unity3D üêü}, 
  year={2019},
  volume={},
  number={},
  pages={1-7},
  abstract={The paper presents URSim: an open source 3D underwater simulation framework for Unmanned Underwater Vehicles (UUVs) developed using Robotics Operating System (ROS) and a real-time game engine called Unity3D. Simulation systems like these enable to implement, test, study and analyze complex systems while minimizing cost and disruption to the environment. URSim provides the user an intuitive way to simulate underwater vehicles and robots. It is capable of simulating feedback control systems, dynamic model, underwater vision and mission planning for underwater vehicles and robots. The simulation provides support for underwater sensor modules, underwater physics, collision kinematics and is highly configurable to simulate a realistic underwater environment. The software architecture is adaptive to algorithms for control systems, image processing, navigation and manipulation.},
  keywords={Three-dimensional displays;Solid modeling;Games;Engines;Cameras;Robots;Physics;Simulation;Underwater Robotics;Unity3D;Robotics Operating System},
  doi={10.1109/UT.2019.8734309},
  code={https://github.com/srmauvsoftware/URSim},
  selected={true},
  html={https://ieeexplore.ieee.org/document/8734309},
  ISSN={2573-3796},
  img={assets/img/publications/srmauv.gif},
}

